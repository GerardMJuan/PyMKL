from typing import Union, List, Tuple, Callable

import itertools
import math
import datetime
import numpy as np
import pandas as pd
from numpy.core.fromnumeric import var
import scipy as sp
import scipy.spatial
import scipy.spatial.distance
import numpy.matlib
import tqdm
import sklearn
import sklearn.cluster
import networkx as nx


def INEXACT_Bermanis_4DATA(Ge_s, f, embedding, direction, e_s, gamma):
    """Implements the Inexact Bermanis interpolation algorithm for data analysis.
    
    Args:
        Ge_s: Gaussian kernel matrix computed from pairwise distances
        f: Input feature values to interpolate
        embedding: Data points in the embedded space
        direction: Direction vectors for interpolation
        e_s: Kernel bandwidth parameter
        gamma: Regularization parameter
    
    Returns:
        dict: Contains interpolated values 'f_s' and 'f_star_s'
    """
    # Get shape of input matrix
    N = Ge_s.shape[0]

    # Solve linear system (Ge_s + I/gamma)c = f
    c = np.linalg.solve((Ge_s + (np.eye(N,N)/gamma)), f.T)

    # Compute interpolation at sample points
    f_s = Ge_s @ c

    # Compute interpolation at query points
    p = direction.shape[0]  # Number of query points
    l = f_s.shape[0]       # Number of sample points
    f_star_s = np.zeros((f_s.shape[1],p))

    # Interpolate for each query point
    for j in range(p):
        # Compute distances to query point
        tmp = np.matlib.repmat(direction[j,:], l ,1)
        tmp = np.sum((tmp - embedding)**2,1).T
        # Compute Gaussian kernel values
        G_star_s = np.exp(-tmp / (2*(e_s)**2))
        
        # Interpolate using computed coefficients
        f_star_s[:,j] = (G_star_s @ c).T

    return {
        "f_s": f_s.T,        # Interpolation at sample points
        "f_star_s": f_star_s # Interpolation at query points
    }



def get_sample(Features: List[Union[np.ndarray, list]], index: int):
    point_features = [f[:,index] for f in Features]

    return point_features

def get_variability_descriptors(embedding: np.ndarray, Features: List[Union[np.ndarray, list]], dimensions: Union[None,int,list] = None, direction: np.ndarray = None, gamma: Union[float,np.ndarray] = None, max_iterations: int = 20, NN_dims: Union[int,None] = None, n_variability_points: int = 5):
    """
    Computes variability descriptors for embedded data using multi-scale kernel regression.
    
    This function implements a multi-scale analysis approach to understand how features
    vary across the embedded space. It uses an iterative kernel regression scheme with
    decreasing bandwidth to capture both global and local feature variations.
    
    Args:
        embedding: Data points in embedded space (n_samples x n_dimensions)
        Features: List of feature arrays to analyze
        dimensions: Which dimensions to analyze (int or list)
        direction: Query points for interpolation (if None, uses standard deviations)
        gamma: Regularization parameter(s)
        max_iterations: Maximum number of multi-scale iterations
        NN_dims: Number of dimensions to use for nearest neighbor calculations
        n_variability_points: Number of points in the variability linspace (default=5)
    
    Returns:
        list: Interpolated feature values at query points for each input feature
    """    
    # Dimensions to array
    if dimensions is None:
        dimensions = embedding.shape[1]
    if isinstance(dimensions,(int,np.integer)):
        dimensions = np.arange(dimensions)
    dimensions = np.array(dimensions)

    # Get subset of interesting dimensions
    embedding_reduced = embedding[:,:NN_dims]

    # Populate optional parameters
    if gamma is None:
        gamma = 1
    if isinstance(gamma,(int,float,np.integer,np.floating)):
        gamma = np.full((len(Features),),gamma)
    gamma = np.array(gamma)

    # Get distance of every projected element to each other
    embedding_distance = sp.spatial.distance.squareform(sp.spatial.distance.pdist(embedding_reduced)) 
    embedding_distance_density = embedding_distance + np.diag(np.full((embedding_reduced.shape[0],),np.inf)) ## put diagonal coefficients to -1
    embedding_distance_density = np.min(embedding_distance_density, 0) ## find closest neighbour
    diameter = np.max(embedding_distance)
    density = np.mean(embedding_distance_density)

    # Iterator depending if it's list (first N dimensions) or list (specific dimensions)
    if isinstance(dimensions,np.integer):
        dimensions = np.arange(dimensions)

    # MSE Algorithm - same in every iteration, take out of loop for efficiency
    if direction is None:
        embedding_mean = np.mean(embedding_reduced,axis=0)
        direction = np.zeros((len(dimensions)*n_variability_points,embedding_reduced.shape[1]))

        for i,d in enumerate(dimensions):
            std = np.std(embedding_reduced[:,d],ddof=1)
            variability = np.linspace(-2*std, 2*std, n_variability_points)

            for j,var in enumerate(variability):
                direction[n_variability_points*i+j,:] = embedding_mean
                direction[n_variability_points*i+j,i] = direction[n_variability_points*i+j,i] + var
    else:
        # If provided, take NN_dims dimensions
        direction = direction.copy()[:,:NN_dims]

    # Initialize outputs
    outputs = []

    # Inexact bermanis? Dig into function
    for n,f in enumerate(tqdm.tqdm(Features)):
        s = 0
        F_s_old = np.zeros_like(f)
        F_star_s_old = np.zeros((f.shape[0],direction.shape[0]))
        while (s <= max_iterations ) & ((diameter/2**s) > (2*density)):
            # Kernel bandwidth
            e_s = diameter/2**s
            Ge_s = np.exp( -embedding_distance**2 / (2*(e_s)**2) );

            # Compute inexact Bermanis
            bermanis = INEXACT_Bermanis_4DATA(Ge_s, f-F_s_old, embedding_reduced, direction, e_s, gamma[n])

            # Update f_s
            F_s = F_s_old + bermanis["f_s"]
            F_s_old = F_s # Store last value

            # Update f_star_s
            F_star_s = F_star_s_old + bermanis["f_star_s"]
            F_star_s_old = F_star_s # Store last value

            # Increment counter
            s = s+1

        F_star = F_star_s_old

        # Store outputs
        outputs.append(F_star)

    return outputs


def regression_line_clusters(embedding: np.ndarray, clusters: Union[int,np.ndarray], dim2sort: int = None, dimensions: Union[None, int, list] = None, random_state: Tuple[int,np.random.RandomState] = None):
    """
    Computes regression lines through cluster centers in embedded space.
    
    Args:
        embedding: Data points in embedded space
        clusters: Either number of clusters to create or pre-computed cluster labels
        dim2sort: Optional dimension to sort clusters by
        dimensions: Which dimensions to analyze
        random_state: Random seed for reproducibility
    
    Returns:
        np.ndarray: Directions/centers for each cluster
    """
    # Treat inputs - dimensions
    if dimensions is None:
        dimensions = embedding.shape[1]
    if isinstance(dimensions,(int,np.integer)):
        dimensions = np.arange(dimensions)
    dimensions = np.array(dimensions)
    # Treat inputs - clusters
    if isinstance(clusters,(int,np.integer)):
        clusters = np.arange(clusters)
        needs_training = True
    else:
        if len(clusters) == embedding.shape[0]:
            labels = clusters # Keep same variable names
            needs_training = False
        else:
            raise ValueError("clusters can only be either the label for each sample or the number of clusters to partition the data in")

    # Auxiliary list of classes for later
    classes = np.unique(clusters)
    
    # Copy embedding (just in case)
    embedding = np.copy(embedding)
        
    # Clusterize data
    direction = np.full((len(classes),len(dimensions)),np.nan)
    if needs_training:
        has_clustered = False
        while not has_clustered:
            index_to_delete = []
            
            # Clusterize data to 'clusters' clusters
            kmeans = sklearn.cluster.KMeans(n_clusters=clusters, n_init=50, random_state=random_state)
            labels = kmeans.fit_predict(embedding[:,dimensions])
            
            for i,c in enumerate(classes):
                condition = (labels == c)
                if np.sum(condition) < 2:
                    index_to_delete.append(np.where(condition)[0])
            
            if len(index_to_delete) == 0:
                has_clustered = True
            else:
                # Sort from largest to smallest index
                index_to_delete = np.concatenate(index_to_delete) # Form a single array
                index_to_delete = np.sort(index_to_delete)[::-1]
                
                for index in index_to_delete:
                    embedding = np.delete(embedding,index,axis=0)
    
    # Fill dimensions
    for i,c in enumerate(classes):
        direction[i,:] = np.mean(embedding[labels == c,:][:,dimensions],axis=0)
        
    if dim2sort is not None:
        direction = direction[np.argsort(direction[:,dim2sort]),:]
    
    return direction
        
    
def regression_line_points(embedding: np.ndarray, point_from: np.ndarray, point_to: np.ndarray, 
                           n_points: int = 10, dimensions: Union[None,int, list] = None, 
                           metric: Callable = np.median):
    """
    Creates a regression line between two points in the embedded space.
    
    Args:
        embedding: Data points in embedded space
        point_from: Starting point coordinates
        point_to: Ending point coordinates
        n_points: Number of interpolation points between start and end
        dimensions: Which dimensions to analyze
        metric: Function to compute central tendency (default: median)
    
    Returns:
        np.ndarray: Array of interpolated points along the regression line
    """

    # Treat inputs - dimensions
    if dimensions is None:
        dimensions = embedding.shape[1]
    if isinstance(dimensions,(int,np.integer)):
        dimensions = np.arange(dimensions)
    dimensions = np.array(dimensions)
    
    # Fill points with mean/median data according to metric
    if point_from.size < dimensions.size:
        point_from = np.concatenate((point_from,metric(embedding[:,point_from.size:],axis=0)))
    if point_to.size   < dimensions.size:
        point_to   = np.concatenate((point_to,  metric(embedding[:,point_to.size:  ],axis=0)))
        
    # Determine the empty direction vector
    direction = (point_to - point_from)/(n_points-1)
    points = np.array([point_from+i*direction for i in range(n_points)])

    return points

def regression_line_dimensions(embedding: np.ndarray, dimension_to_explore: int, n_points: int = 10, dimensions: Union[None,int, list] = None, metric: Callable = np.median):
    """Computes regression lines along a specific dimension in the embedded space.
    
    Args:
        embedding: Data points in embedded space
        dimension_to_explore: Which dimension to analyze
        n_points: Number of points to sample along the dimension
        dimensions: Which dimensions to include in analysis
        metric: Function to compute central tendency (default: median)
    
    Returns:
        np.ndarray: Direction vectors for the regression line
    """

    # Treat inputs - dimensions
    if dimensions is None:
        dimensions = embedding.shape[1]
    if isinstance(dimensions,(int,np.integer)):
        dimensions = np.arange(dimensions)
    dimensions = np.array(dimensions)
    
    # Determine the empty direction vector
    direction = np.full((n_points,len(dimensions)),np.nan)
    embedding_reduced = embedding[:,dimension_to_explore]
    boundaries = np.linspace(np.min(embedding_reduced),np.max(embedding_reduced),n_points+1)

    for i,n in enumerate(range(n_points)):
        condition_boundaries = (embedding_reduced>boundaries[i]) & (embedding_reduced<boundaries[i+1])
        direction[i,:] = metric(embedding[condition_boundaries,:][:,dimensions],axis=0)

    return direction


def regression_line_std(embedding: np.ndarray, n_points: int = 5, dimensions: Union[None,int, list] = None):
    """Computes regression lines based on standard deviations from the mean.
    
    Args:
        embedding: Data points in embedded space
        n_points: Number of points to sample along each dimension
        dimensions: Which dimensions to analyze
    
    Returns:
        np.ndarray: Direction vectors based on standard deviation intervals
    """    
    # Treat inputs - dimensions
    if dimensions is None:
        dimensions = embedding.shape[1]
    if isinstance(dimensions,(int,np.integer)):
        dimensions = np.arange(dimensions)
    dimensions = np.array(dimensions)
    
    # MSE Algorithm - same in every iteration, take out of loop for efficiency
    embedding_mean = np.mean(embedding,axis=0)
    direction = np.matlib.repmat(embedding_mean,len(dimensions)*n_points,1)
    for i,d in enumerate(dimensions):
        std = np.std(embedding[:,d],ddof=1)
        variability = np.linspace(-2*std,2*std,n_points)

        for j,var in enumerate(variability):
            direction[n_points*i+j,i] = direction[n_points*i+j,i] + var
        
    return direction



def embedding_self_correlation(embedding: np.ndarray, display_dimensions: bool = True, correlation_threshold: float = 0.9, metric: Callable = lambda x: scipy.spatial.distance.squareform(scipy.spatial.distance.pdist(x,metric="euclidean"))) -> Tuple[np.ndarray,np.ndarray]:
    """
    This code is used to assess the number of relevant dimensions to
    consider from an MKL output space. To do so, it ranks the neighbors for
    each of the data entry (patient) and compares it to the same ranking but
    computed with one more dimension. This way, when the ranking of nearest
    neighbors does not change, we can argue that the space has stabilized.
    
    Inputs:
    * embedding: embedding of the input data in the latent space
    """
    
    # Retrieve input shape
    N,M = embedding.shape
    M_min = M # Initialize to have the same amount of dimensions to analyze

    self_correlation = np.zeros((M,))

    # Iterator to display progress
    try:
        import tqdm
        iterator = tqdm.tqdm(range(1,M+1))
    except ModuleNotFoundError:
        iterator = range(1,M+1)

    for dim in iterator:
        if dim == 1: # Keep tqdm with M iterations for easier understanding
            continue
            
        # ???
        rank_vectors = np.zeros((N,))

        # Compute euclidean distance matrices of 'dim' and 'dim+1' dimensions
        k1 = metric(embedding[:,:dim-1])
        k2 = metric(embedding[:,:dim])

        # Sort pairwise distances
        argsort_k1 = np.argsort(k1)
        argsort_k2 = np.argsort(np.argsort(k2))

        # Save up time
        arange = np.arange(N)

        # Compute rankings for the different number of dimensions
        for aux_dim in range(N):
            # Take index in the value-sorted embedding
            sort_k1 = argsort_k1[aux_dim,:]
            sort_k2 = argsort_k2[aux_dim,:]

            # Sort k2 w.r.t. k1
            sort_k2 = sort_k2[sort_k1]

            # Order based Spearman/Kendall rank correlation
            rank_vectors[aux_dim] = sp.stats.spearmanr(arange,sort_k2)[0]

        # Set minimum number of dimensions when correlation is over 'correlation_threshold'
        self_correlation[dim-1] = np.mean(rank_vectors) # Average the correlation of all patient rankings
        if (self_correlation[dim-1] >= correlation_threshold):
            M_min = dim-1
            break

    return self_correlation,M_min


def cluster_MKR(embedding: np.ndarray, Features: List[np.ndarray], clusters: np.ndarray, dimensions: Union[None,int,np.ndarray] = None, NN_dims: Union[int,None] = None, return_embeddings: bool = False):
    """
    Performs multi-kernel regression analysis on clustered data.
    
    For each cluster, this function:
    1. Centers the cluster data
    2. Performs PCA
    3. Computes variability descriptors
    
    Args:
        embedding: Data points in embedded space
        Features: List of feature arrays to analyze
        clusters: Cluster assignments for each point
        dimensions: Which dimensions to analyze
        NN_dims: Number of dimensions for nearest neighbor calculations
        return_embeddings: Whether to return transformed embeddings
        
    Returns:
        Union[List, Tuple]: Descriptors and optionally embeddings per cluster
    """
    # Treat inputs - dimensions
    if dimensions is None:
        dimensions = embedding.shape[1]
    if isinstance(dimensions,(int,np.integer)):
        dimensions = np.arange(dimensions)
    dimensions = np.array(dimensions)

    # Output structures
    descriptors = []
    out_embeddings = []

    print("test")

    # Obtain the per-cluster main directions and descriptors
    for i,c in enumerate(np.unique(clusters)):
        # Retrieve samples that belong to specific cluster
        filter_cluster = (clusters == c)

        # Take the elements corresponding to a certain cluster and center at zero
        embedding_cluster = embedding[filter_cluster,:]
        embedding_cluster -= np.matlib.repmat(np.mean(embedding_cluster,axis=0),embedding_cluster.shape[0],1)

        # Compute PCA
        _,V = np.linalg.eigh(np.cov(embedding_cluster.T),UPLO="U")
        V = np.fliplr(V).T
        embedding_PCA = (V @ embedding_cluster.T).T

        # Get features per cluster
        Features_cluster = [f[:,filter_cluster] for f in Features]

        # Get variability descriptors
        d = get_variability_descriptors(embedding_PCA,Features_cluster,dimensions=dimensions,NN_dims=NN_dims)

        # Save outputs
        descriptors.append(d)
        out_embeddings.append(embedding_PCA)

    if return_embeddings:
        return descriptors,out_embeddings
    else:
        return descriptors


def compute_agreement(input, target):
    """
    Calculates clustering accuracy by finding optimal label matching.
    
    Tries all possible permutations of cluster labels to find the arrangement
    that best matches the target labels. Note: becomes computationally expensive
    for more than 7 clusters.
    
    Function for calculating clustering accuray and matching found 
    labels with true labels. Assumes input and target both are Nx1 vectors with
    clustering labels. Does not support fuzzy clustering.
    
    Algorithm is based on trying out all reorderings of cluster labels, 
    e.g. if input = [1 2 2], it tries [1 2 2] and [2 1 1] so see which fits
    best the truth vector. Since this approach makes use of perms(),
    the code will not run for unique(input) greater than 10, and it will slow
    down significantly for number of clusters greater than 7.
    
    Input:
      input  - result from clustering (y-test)
      target - truth vector
    
    Output:
      accuracy    -   Overall accuracy for entire clustering (OA). For
                      overall error, use OE = 1 - OA.
      true_labels -   Vector giving the label rearangement witch best 
                      match the truth vector (target).
      CM          -   Confusion matrix. If unique(input) = 4, produce a
                      4x4 matrix of the number of different errors and  
                      correct clusterings done."""
    
    N = target.size

    # Get the number of clusters
    clusters = np.unique(input)

    # Get all possible permutations of available cluster IDs
    permutations = np.vstack(list(itertools.permutations(clusters)))[::-1]

    # Check which permutation yields the closest response to any given cluster
    agreements = []
    for i,permutation in enumerate(permutations):
        # Reassign the labels to a certain permutation of elements
        flipped_labels = permutation[np.argmax((input == permutation[:,None])[::-1],axis=0)]

        # Save value
        agreements.append(np.sum(flipped_labels == target)/N)

    return agreements,np.max(agreements)


def process_table(data: pd.DataFrame, clusters: np.ndarray, y: Union[str,np.ndarray,list] = "outcome", dtypes: dict = {}):
    """
    Generates statistical summary of variables grouped by clusters.
    
    For each variable:
    - Determines appropriate statistical test based on data type
    - Computes descriptive statistics per cluster
    - Performs statistical significance testing between clusters
    
    Args:
        data: DataFrame containing variables to analyze
        clusters: Cluster assignments
        y: Target variable name or array
        dtypes: Dictionary of data types for variables
        
    Returns:
        pd.DataFrame: Summary statistics and p-values for each variable
    """

    
    # Check inputs
    if isinstance(y, str):
        if y not in data:
            raise KeyError("if y is a string, the Dataframe object *must* contain said key")
    else:
        if data.shape[0] != len(y):
            raise ValueError("if y is an array, the length of the vector must match the number of samples")
    
    # Copy data just in case
    data = data.copy()
    dtypes = dtypes.copy()

    # Get variable types for checking statistical test to be used
    var_types = {}

    # Skip these dtypes
    skip_dtypes = (datetime.datetime, np.datetime64)

    for k in data:
        if k in dtypes:
            continue
        
        mark_break = False
        for v in data[k]:
            if isinstance(v,skip_dtypes):
                mark_break = True

        if mark_break:
            var_types[k] = None
            continue
    
        # Cast if dtype is object
        if data[k].dtype == np.object_:
            try:
                data[k] = pd.to_numeric(data[k])
            except ValueError:
                var_types[k] = None
                continue

        values = data[k][~np.isnan(data[k])]
        unique_values = np.unique(values)

        # Check if values are categorical (no decimal numbers)
        if np.allclose(unique_values,unique_values.astype(int).astype(values.dtype)):
            if unique_values.size == 2:
                var_types[k] = bool
            else:
                var_types[k] = int
        else:
            var_types[k] = float
        
    # Get filters for every cluster
    filters_cluster = {c: (clusters == c) for c in np.unique(clusters)}
    
    # Iterate over variables
    outputs = {}#[None for _ in range(len(data_dict.keys()))]
    for i,k in enumerate(data):
        # Skip if not castable
        if var_types[k] is None:
            continue

        # Add space for information
        outputs[k] = {}

        # Get non-NaN values
        if var_types[k] == bool:
            non_nan_data = data[k][~np.isnan(data[k])].values.astype(float)
            non_nan_data = (non_nan_data-non_nan_data.min()).astype(var_types[k])
        else:
            non_nan_data = data[k][~np.isnan(data[k])].values.astype(var_types[k])
        non_nan_clusters = clusters[~np.isnan(data[k])]
        if (non_nan_data.size == 0) or (non_nan_clusters.size == 0):
            outputs[k]["p-value"] = np.NaN

            for c in np.unique(clusters):
                outputs[k][f"Cluster {c} (n = {np.sum(filters_cluster[c])})"] = f"---"
            continue
        is_normal = sp.stats.kstest(sp.stats.zscore(non_nan_data),"norm").pvalue >= 5e-2
        
        # Check ranges
        groups = []
        for c in np.unique(clusters):
            # Get elements that coincide with the cluster
            cluster_data         = data[k].values[filters_cluster[c]]
            if var_types[k] == bool:
                non_nan_cluster_data = cluster_data[~np.isnan(cluster_data)].astype(float)
                if (np.unique(data[k].values).size == 2):
                    non_nan_cluster_data = (non_nan_cluster_data-np.unique(data[k].values).min()).astype(var_types[k])
            else:
                non_nan_cluster_data = cluster_data[~np.isnan(cluster_data)].astype(var_types[k])

            if (cluster_data.size == 0) or (non_nan_cluster_data.size == 0):
                outputs[k][f"Cluster {c} (n = {np.sum(filters_cluster[c])})"] = f"---"
                continue
            
            # Get groups of data
            groups.append(non_nan_cluster_data)
            
            if var_types[k] is bool: # Case binary
                to_bool      = (non_nan_cluster_data == True)
                outputs[k][f"Cluster {c} (n = {np.sum(filters_cluster[c])})"] = f"{to_bool.sum()} ({np.round(100*np.mean(to_bool),2)})%"
            elif is_normal:
                mean         = np.mean(non_nan_cluster_data)
                std          = np.std(non_nan_cluster_data,ddof=1)
                outputs[k][f"Cluster {c} (n = {np.sum(filters_cluster[c])})"] = f"{np.round(mean,2)} ± {np.round(std,2)}"
            else:
                median       = np.median(non_nan_cluster_data)
                y_25         = np.quantile(non_nan_cluster_data,0.25, interpolation='midpoint')
                y_75         = np.quantile(non_nan_cluster_data,0.75, interpolation='midpoint')
                outputs[k][f"Cluster {c} (n = {np.sum(filters_cluster[c])})"] = f"{np.round(median,2)} ({np.round(y_25,2)} to {np.round(y_75,2)})"
    
        # Compute p-values
        if (non_nan_data.size == 0) or (non_nan_clusters.size == 0):
            outputs[k]["p-value"] = np.NaN
            continue

        if (var_types[k] is bool) or (k == y):
            # Compute chi-square test for statistical significance among groups
            crosstab = pd.crosstab(data[k].values,clusters).values
            _,outputs[k]["p-value"],_,_ = sp.stats.chi2_contingency(crosstab, correction=False)
        else:
            if (len(groups) < 2):
                outputs[k]["p-value"] = np.NaN
                continue
            elif is_normal: # If is normal, Kruskal-Wallis test
                _,outputs[k]["p-value"] = sp.stats.kruskal(*groups)
            else:
                _,outputs[k]["p-value"] = sp.stats.f_oneway(*groups)

        # if k == y:
        #     return non_nan_data,non_nan_clusters
            
    return pd.DataFrame(outputs).T

def get_graph(embedding: np.ndarray, N_neighbours: int = 5, NN_dims: Union[int,None] = None, labels: list = None):
    """
    Constructs a nearest neighbor graph from embedded data points.
    
    Creates an undirected graph where:
    - Nodes represent data points
    - Edges connect to N nearest neighbors
    - Edge weights are distances between points
    
    Args:
        embedding: Data points in embedded space
        N_neighbours: Number of nearest neighbors to connect
        NN_dims: Dimensions to use for distance calculations
        labels: Optional node labels
        
    Returns:
        nx.Graph: NetworkX graph of nearest neighbor connections
    """
    # Dimensions to array
    if NN_dims is None:
        NN_dims = embedding.shape[1]
    if isinstance(NN_dims,(int,np.integer)):
        NN_dims = np.arange(NN_dims)
    NN_dims = np.array(NN_dims)

    # Get subset of interesting dimensions
    embedding_reduced = embedding[:,NN_dims]

    # Get distance of every projected element to each other
    embedding_distance  = sp.spatial.distance.squareform(sp.spatial.distance.pdist(embedding_reduced)) 
    embedding_distance += np.diag(np.full((embedding_reduced.shape[0],),np.inf)) ## put diagonal coefficients to -1
     
    # Find N neighbours
    neighbours          = np.argsort(embedding_distance,axis=1)[:,:N_neighbours]
    
    # Create graph
    g = nx.Graph()
    for i,nn in enumerate(neighbours):
        for j in nn:
            g.add_edge(i,j,weight=embedding_distance[i,j])
    if labels is not None:
        nx.set_node_attributes(g,{i: {"label": v} for i,v in enumerate(labels)})
    
    return g